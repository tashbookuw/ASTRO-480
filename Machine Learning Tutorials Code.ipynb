{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "print(type(iris))\n",
    "print(type(digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(digits.data[:-1], digits.target[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(digits.data[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pickle.dumps(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = pickle.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.predict(X[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(clf, 'filename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('filename.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import random_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.rand(10, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = random_projection.GaussianRandomProjection()\n",
    "X_new = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clf.predict(iris.data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(iris.data, iris.target_names[iris.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'setosa', 'setosa']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(clf.predict(iris.data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.rand(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rng.binomial (1, .5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = rng.rand(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.set_params(kernel='linear').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " clf.set_params(kernel='rbf').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 2], [2, 4], [4, 5], [3, 2], [3, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0, 0, 1, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "classif = OneVsRestClassifier(estimator=SVC(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = LabelBinarizer().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[0, 1], [0, 2], [1, 3], [0, 2, 3], [2, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = MultiLabelBinarizer().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classif.fit(X, y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of introductory tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: djohnson@cs.ucsd.edu (Darin Johnson)\n",
      "Subject: Re: harrassed at work, could use some prayers\n",
      "Organization: =CSE Dept., U.C. San Diego\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[2].split(\"\\n\")[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "for t in twenty_train.target[:10]:print(twenty_train.target_names[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: revdak@netcom.com (D. Andrew Kille)\\nSubject: Re: Easter: what\\'s in a name? (was Re: New Testament Double Stan\\nOrganization: NETCOM On-line Communication Services (408 241-9760 guest)\\nLines: 40\\n\\nDaniel Segard (dsegard@nyx.cs.du.edu) wrote:\\n\\n[a lot of stuff deleted]\\n\\n:      For that matter, stay Biblical and call it Omar Rasheet (The Feast of\\n: First Fruits).  Torah commands that this be observed on the day following\\n: the Sabbath of Passover week.  (Sunday by any other name in modern\\n: parlance.)  Why is there so much objection to observing the Resurrection\\n: on the 1st day of the week on which it actually occured?  Why jump it all\\n: over the calendar the way Easter does?  Why not just go with the Sunday\\n: following Passover the way the Bible has it?  Why seek after unbiblical\\n: methods?\\n:  \\nIn fact, that is the reason Easter \"jumps all over the calendar\"- Passsover\\nitself is a lunar holiday, not a solar one, and thus falls over a wide\\npossible span of times.  The few times that Easter does not fall during or\\nafter Passover are because Easter is further linked to the Vernal Equinox-\\nthe beginning of spring.\\n\\n[more deletions]\\n:  \\n:       So what does this question have to do with Easter (the whore\\n: goddess)?  I am all for celebrating the Resurrection.  Just keep that\\n: whore out of the discussion.\\n:  \\nYour obsession with the term \"whore\" clouds your argument.  \"Whore\" is\\na value judgement, not a descriptive term.\\n\\n[more deletions]\\n\\nOverall, this argument is an illustration of the \"etymological fallacy\"\\n(see J.P. Louw: _Semantics of NT Greek_).  That is the idea that the true\\nmeaning of a word lies in its origins and linguistic form.  In fact, our\\nown experience demonstrates that the meaning of a word is bound up with\\nhow it is _used_, not where it came from.  Very few modern people would\\nmake any connection whatsoever between \"Easter\" and \"Ishtar.\"  If Daniel\\nSeagard does, then for him it has that meaning.  But that is a highly\\nidiosyncratic \"meaning,\" and not one that needs much refutation.\\n\\nrevdak@netcom.com\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.data[68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yo that's dope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_counts = count_vect.transform(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "     categories=categories, shuffle=True, random_state=42)\n",
    "#print(twenty_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = twenty_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == twenty_test.target)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=5, tol=None)),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(twenty_train.data, twenty_train.target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9127829560585885"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I feel like it's kind of late to be importing metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "     target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[258,  11,  15,  35],\n",
       "       [  4, 379,   3,   3],\n",
       "       [  5,  33, 355,   3],\n",
       "       [  5,  10,   4, 379]], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'tfidf__use_idf': (True, False),\n",
    "               'clf__alpha': (1e-2, 1e-3),\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[gs_clf.predict(['Jesus works for HP'])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters.keys()):\n",
    "     print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done with tutorials -- moving on to example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ar.wikipedia.org/wiki/%D9%88%D9%8A%D9%83%D9%8A%D8%A8%D9%8A%D8%AF%D9%8A%D8%A7\n",
      "Writing paragraphs\\ar\\ar_0000.txt\n",
      "Writing short_paragraphs\\ar\\ar_0000.txt\n",
      "Writing short_paragraphs\\ar\\ar_0001.txt\n",
      "Writing short_paragraphs\\ar\\ar_0002.txt\n",
      "Writing short_paragraphs\\ar\\ar_0003.txt\n",
      "Writing short_paragraphs\\ar\\ar_0004.txt\n",
      "Writing short_paragraphs\\ar\\ar_0005.txt\n",
      "Writing short_paragraphs\\ar\\ar_0006.txt\n",
      "Writing short_paragraphs\\ar\\ar_0007.txt\n",
      "Writing short_paragraphs\\ar\\ar_0008.txt\n",
      "Writing short_paragraphs\\ar\\ar_0009.txt\n",
      "Writing short_paragraphs\\ar\\ar_0010.txt\n",
      "Writing paragraphs\\ar\\ar_0001.txt\n",
      "Writing short_paragraphs\\ar\\ar_0011.txt\n",
      "Writing short_paragraphs\\ar\\ar_0012.txt\n",
      "Writing short_paragraphs\\ar\\ar_0013.txt\n",
      "Writing short_paragraphs\\ar\\ar_0014.txt\n",
      "Writing short_paragraphs\\ar\\ar_0015.txt\n",
      "Writing short_paragraphs\\ar\\ar_0016.txt\n",
      "Writing short_paragraphs\\ar\\ar_0017.txt\n",
      "Writing short_paragraphs\\ar\\ar_0018.txt\n",
      "Writing short_paragraphs\\ar\\ar_0019.txt\n",
      "Writing short_paragraphs\\ar\\ar_0020.txt\n",
      "Writing short_paragraphs\\ar\\ar_0021.txt\n",
      "Writing short_paragraphs\\ar\\ar_0022.txt\n",
      "Writing short_paragraphs\\ar\\ar_0023.txt\n",
      "Writing short_paragraphs\\ar\\ar_0024.txt\n",
      "Writing short_paragraphs\\ar\\ar_0025.txt\n",
      "Writing short_paragraphs\\ar\\ar_0026.txt\n",
      "Writing short_paragraphs\\ar\\ar_0027.txt\n",
      "Writing short_paragraphs\\ar\\ar_0028.txt\n",
      "Writing short_paragraphs\\ar\\ar_0029.txt\n",
      "Writing short_paragraphs\\ar\\ar_0030.txt\n",
      "Writing short_paragraphs\\ar\\ar_0031.txt\n",
      "Writing paragraphs\\ar\\ar_0002.txt\n",
      "Writing short_paragraphs\\ar\\ar_0032.txt\n",
      "Writing short_paragraphs\\ar\\ar_0033.txt\n",
      "Writing short_paragraphs\\ar\\ar_0034.txt\n",
      "Writing short_paragraphs\\ar\\ar_0035.txt\n",
      "Writing short_paragraphs\\ar\\ar_0036.txt\n",
      "Writing short_paragraphs\\ar\\ar_0037.txt\n",
      "Writing short_paragraphs\\ar\\ar_0038.txt\n",
      "Writing short_paragraphs\\ar\\ar_0039.txt\n",
      "Writing short_paragraphs\\ar\\ar_0040.txt\n",
      "Writing short_paragraphs\\ar\\ar_0041.txt\n",
      "Writing short_paragraphs\\ar\\ar_0042.txt\n",
      "Writing short_paragraphs\\ar\\ar_0043.txt\n",
      "Writing short_paragraphs\\ar\\ar_0044.txt\n",
      "Writing short_paragraphs\\ar\\ar_0045.txt\n",
      "Writing short_paragraphs\\ar\\ar_0046.txt\n",
      "Writing short_paragraphs\\ar\\ar_0047.txt\n",
      "Writing short_paragraphs\\ar\\ar_0048.txt\n",
      "Writing short_paragraphs\\ar\\ar_0049.txt\n",
      "Writing short_paragraphs\\ar\\ar_0050.txt\n",
      "Writing short_paragraphs\\ar\\ar_0051.txt\n",
      "Writing short_paragraphs\\ar\\ar_0052.txt\n",
      "Writing short_paragraphs\\ar\\ar_0053.txt\n",
      "Writing paragraphs\\ar\\ar_0003.txt\n",
      "Writing short_paragraphs\\ar\\ar_0054.txt\n",
      "Writing short_paragraphs\\ar\\ar_0055.txt\n",
      "Writing short_paragraphs\\ar\\ar_0056.txt\n",
      "Writing short_paragraphs\\ar\\ar_0057.txt\n",
      "Writing short_paragraphs\\ar\\ar_0058.txt\n",
      "Writing short_paragraphs\\ar\\ar_0059.txt\n",
      "Writing paragraphs\\ar\\ar_0004.txt\n",
      "Writing short_paragraphs\\ar\\ar_0060.txt\n",
      "Writing short_paragraphs\\ar\\ar_0061.txt\n",
      "Writing short_paragraphs\\ar\\ar_0062.txt\n",
      "Writing short_paragraphs\\ar\\ar_0063.txt\n",
      "Writing short_paragraphs\\ar\\ar_0064.txt\n",
      "Writing short_paragraphs\\ar\\ar_0065.txt\n",
      "Writing short_paragraphs\\ar\\ar_0066.txt\n",
      "Writing short_paragraphs\\ar\\ar_0067.txt\n",
      "Writing short_paragraphs\\ar\\ar_0068.txt\n",
      "Writing short_paragraphs\\ar\\ar_0069.txt\n",
      "Writing short_paragraphs\\ar\\ar_0070.txt\n",
      "Writing short_paragraphs\\ar\\ar_0071.txt\n",
      "Writing short_paragraphs\\ar\\ar_0072.txt\n",
      "Writing short_paragraphs\\ar\\ar_0073.txt\n",
      "Writing short_paragraphs\\ar\\ar_0074.txt\n",
      "Writing short_paragraphs\\ar\\ar_0075.txt\n",
      "Writing short_paragraphs\\ar\\ar_0076.txt\n",
      "Writing short_paragraphs\\ar\\ar_0077.txt\n",
      "Writing short_paragraphs\\ar\\ar_0078.txt\n",
      "Writing paragraphs\\ar\\ar_0005.txt\n",
      "Writing short_paragraphs\\ar\\ar_0079.txt\n",
      "Writing short_paragraphs\\ar\\ar_0080.txt\n",
      "Writing short_paragraphs\\ar\\ar_0081.txt\n",
      "Writing short_paragraphs\\ar\\ar_0082.txt\n",
      "Writing short_paragraphs\\ar\\ar_0083.txt\n",
      "Writing short_paragraphs\\ar\\ar_0084.txt\n",
      "Writing short_paragraphs\\ar\\ar_0085.txt\n",
      "Writing short_paragraphs\\ar\\ar_0086.txt\n",
      "Writing short_paragraphs\\ar\\ar_0087.txt\n",
      "Writing short_paragraphs\\ar\\ar_0088.txt\n",
      "Writing short_paragraphs\\ar\\ar_0089.txt\n",
      "Writing short_paragraphs\\ar\\ar_0090.txt\n",
      "Writing short_paragraphs\\ar\\ar_0091.txt\n",
      "Writing short_paragraphs\\ar\\ar_0092.txt\n",
      "Writing paragraphs\\ar\\ar_0006.txt\n",
      "Writing short_paragraphs\\ar\\ar_0093.txt\n",
      "Writing short_paragraphs\\ar\\ar_0094.txt\n",
      "Writing short_paragraphs\\ar\\ar_0095.txt\n",
      "Writing short_paragraphs\\ar\\ar_0096.txt\n",
      "Writing short_paragraphs\\ar\\ar_0097.txt\n",
      "Writing short_paragraphs\\ar\\ar_0098.txt\n",
      "Writing short_paragraphs\\ar\\ar_0099.txt\n",
      "Writing short_paragraphs\\ar\\ar_0100.txt\n",
      "Writing short_paragraphs\\ar\\ar_0101.txt\n",
      "Writing short_paragraphs\\ar\\ar_0102.txt\n",
      "Writing paragraphs\\ar\\ar_0007.txt\n",
      "Writing short_paragraphs\\ar\\ar_0103.txt\n",
      "Writing short_paragraphs\\ar\\ar_0104.txt\n",
      "Writing short_paragraphs\\ar\\ar_0105.txt\n",
      "Writing short_paragraphs\\ar\\ar_0106.txt\n",
      "Writing paragraphs\\ar\\ar_0008.txt\n",
      "Writing short_paragraphs\\ar\\ar_0107.txt\n",
      "Writing short_paragraphs\\ar\\ar_0108.txt\n",
      "Writing short_paragraphs\\ar\\ar_0109.txt\n",
      "Writing short_paragraphs\\ar\\ar_0110.txt\n",
      "Writing short_paragraphs\\ar\\ar_0111.txt\n",
      "Writing short_paragraphs\\ar\\ar_0112.txt\n",
      "Writing short_paragraphs\\ar\\ar_0113.txt\n",
      "Writing short_paragraphs\\ar\\ar_0114.txt\n",
      "Writing short_paragraphs\\ar\\ar_0115.txt\n",
      "Writing short_paragraphs\\ar\\ar_0116.txt\n",
      "Writing short_paragraphs\\ar\\ar_0117.txt\n",
      "Writing paragraphs\\ar\\ar_0009.txt\n",
      "Writing short_paragraphs\\ar\\ar_0118.txt\n",
      "Writing short_paragraphs\\ar\\ar_0119.txt\n",
      "Writing short_paragraphs\\ar\\ar_0120.txt\n",
      "Writing short_paragraphs\\ar\\ar_0121.txt\n",
      "Writing short_paragraphs\\ar\\ar_0122.txt\n",
      "Writing short_paragraphs\\ar\\ar_0123.txt\n",
      "Writing short_paragraphs\\ar\\ar_0124.txt\n",
      "Writing short_paragraphs\\ar\\ar_0125.txt\n",
      "Writing short_paragraphs\\ar\\ar_0126.txt\n",
      "Writing short_paragraphs\\ar\\ar_0127.txt\n",
      "Writing short_paragraphs\\ar\\ar_0128.txt\n",
      "Writing short_paragraphs\\ar\\ar_0129.txt\n",
      "Writing short_paragraphs\\ar\\ar_0130.txt\n",
      "Writing short_paragraphs\\ar\\ar_0131.txt\n",
      "Writing short_paragraphs\\ar\\ar_0132.txt\n",
      "Writing short_paragraphs\\ar\\ar_0133.txt\n",
      "Writing short_paragraphs\\ar\\ar_0134.txt\n",
      "Writing short_paragraphs\\ar\\ar_0135.txt\n",
      "Writing short_paragraphs\\ar\\ar_0136.txt\n",
      "Writing short_paragraphs\\ar\\ar_0137.txt\n",
      "Writing short_paragraphs\\ar\\ar_0138.txt\n",
      "Writing short_paragraphs\\ar\\ar_0139.txt\n",
      "Writing short_paragraphs\\ar\\ar_0140.txt\n",
      "Writing short_paragraphs\\ar\\ar_0141.txt\n",
      "Writing short_paragraphs\\ar\\ar_0142.txt\n",
      "Writing paragraphs\\ar\\ar_0010.txt\n",
      "Writing short_paragraphs\\ar\\ar_0143.txt\n",
      "Writing short_paragraphs\\ar\\ar_0144.txt\n",
      "Writing short_paragraphs\\ar\\ar_0145.txt\n",
      "Writing short_paragraphs\\ar\\ar_0146.txt\n",
      "Writing short_paragraphs\\ar\\ar_0147.txt\n",
      "Writing paragraphs\\ar\\ar_0011.txt\n",
      "Writing short_paragraphs\\ar\\ar_0148.txt\n",
      "Writing short_paragraphs\\ar\\ar_0149.txt\n",
      "Writing short_paragraphs\\ar\\ar_0150.txt\n",
      "Writing short_paragraphs\\ar\\ar_0151.txt\n",
      "Writing paragraphs\\ar\\ar_0012.txt\n",
      "Writing short_paragraphs\\ar\\ar_0152.txt\n",
      "Writing short_paragraphs\\ar\\ar_0153.txt\n",
      "Writing short_paragraphs\\ar\\ar_0154.txt\n",
      "Writing short_paragraphs\\ar\\ar_0155.txt\n",
      "Writing short_paragraphs\\ar\\ar_0156.txt\n",
      "Writing paragraphs\\ar\\ar_0013.txt\n",
      "Writing short_paragraphs\\ar\\ar_0157.txt\n",
      "Writing short_paragraphs\\ar\\ar_0158.txt\n",
      "Writing short_paragraphs\\ar\\ar_0159.txt\n",
      "Writing short_paragraphs\\ar\\ar_0160.txt\n",
      "Writing paragraphs\\ar\\ar_0014.txt\n",
      "Writing short_paragraphs\\ar\\ar_0161.txt\n",
      "Writing short_paragraphs\\ar\\ar_0162.txt\n",
      "Writing short_paragraphs\\ar\\ar_0163.txt\n",
      "Writing short_paragraphs\\ar\\ar_0164.txt\n",
      "Writing short_paragraphs\\ar\\ar_0165.txt\n",
      "Writing paragraphs\\ar\\ar_0015.txt\n",
      "Writing short_paragraphs\\ar\\ar_0166.txt\n",
      "Writing short_paragraphs\\ar\\ar_0167.txt\n",
      "Writing short_paragraphs\\ar\\ar_0168.txt\n",
      "Writing short_paragraphs\\ar\\ar_0169.txt\n",
      "Writing short_paragraphs\\ar\\ar_0170.txt\n",
      "Writing short_paragraphs\\ar\\ar_0171.txt\n",
      "Writing short_paragraphs\\ar\\ar_0172.txt\n",
      "Writing short_paragraphs\\ar\\ar_0173.txt\n",
      "Writing short_paragraphs\\ar\\ar_0174.txt\n",
      "Writing short_paragraphs\\ar\\ar_0175.txt\n",
      "Writing paragraphs\\ar\\ar_0016.txt\n",
      "Writing short_paragraphs\\ar\\ar_0176.txt\n",
      "Writing short_paragraphs\\ar\\ar_0177.txt\n",
      "Writing short_paragraphs\\ar\\ar_0178.txt\n",
      "Writing short_paragraphs\\ar\\ar_0179.txt\n",
      "Writing short_paragraphs\\ar\\ar_0180.txt\n",
      "Writing short_paragraphs\\ar\\ar_0181.txt\n",
      "Writing paragraphs\\ar\\ar_0017.txt\n",
      "Writing short_paragraphs\\ar\\ar_0182.txt\n",
      "Writing short_paragraphs\\ar\\ar_0183.txt\n",
      "Writing short_paragraphs\\ar\\ar_0184.txt\n",
      "Writing short_paragraphs\\ar\\ar_0185.txt\n",
      "Writing short_paragraphs\\ar\\ar_0186.txt\n",
      "Writing short_paragraphs\\ar\\ar_0187.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing short_paragraphs\\ar\\ar_0188.txt\n",
      "Writing short_paragraphs\\ar\\ar_0189.txt\n",
      "Writing short_paragraphs\\ar\\ar_0190.txt\n",
      "Writing short_paragraphs\\ar\\ar_0191.txt\n",
      "Writing short_paragraphs\\ar\\ar_0192.txt\n",
      "Writing short_paragraphs\\ar\\ar_0193.txt\n",
      "Writing paragraphs\\ar\\ar_0018.txt\n",
      "Writing short_paragraphs\\ar\\ar_0194.txt\n",
      "Writing short_paragraphs\\ar\\ar_0195.txt\n",
      "Writing short_paragraphs\\ar\\ar_0196.txt\n",
      "Writing short_paragraphs\\ar\\ar_0197.txt\n",
      "Writing short_paragraphs\\ar\\ar_0198.txt\n",
      "Writing paragraphs\\ar\\ar_0019.txt\n",
      "Writing short_paragraphs\\ar\\ar_0199.txt\n",
      "Writing short_paragraphs\\ar\\ar_0200.txt\n",
      "Writing short_paragraphs\\ar\\ar_0201.txt\n",
      "Writing short_paragraphs\\ar\\ar_0202.txt\n",
      "Writing short_paragraphs\\ar\\ar_0203.txt\n",
      "Writing short_paragraphs\\ar\\ar_0204.txt\n",
      "Writing short_paragraphs\\ar\\ar_0205.txt\n",
      "Writing short_paragraphs\\ar\\ar_0206.txt\n",
      "Writing paragraphs\\ar\\ar_0020.txt\n",
      "Writing short_paragraphs\\ar\\ar_0207.txt\n",
      "Writing short_paragraphs\\ar\\ar_0208.txt\n",
      "Writing short_paragraphs\\ar\\ar_0209.txt\n",
      "Writing short_paragraphs\\ar\\ar_0210.txt\n",
      "Writing short_paragraphs\\ar\\ar_0211.txt\n",
      "Writing short_paragraphs\\ar\\ar_0212.txt\n",
      "Writing short_paragraphs\\ar\\ar_0213.txt\n",
      "Writing short_paragraphs\\ar\\ar_0214.txt\n",
      "Writing short_paragraphs\\ar\\ar_0215.txt\n",
      "Writing paragraphs\\ar\\ar_0021.txt\n",
      "Writing short_paragraphs\\ar\\ar_0216.txt\n",
      "Writing short_paragraphs\\ar\\ar_0217.txt\n",
      "Writing short_paragraphs\\ar\\ar_0218.txt\n",
      "Writing short_paragraphs\\ar\\ar_0219.txt\n",
      "Writing short_paragraphs\\ar\\ar_0220.txt\n",
      "Writing paragraphs\\ar\\ar_0022.txt\n",
      "Writing short_paragraphs\\ar\\ar_0221.txt\n",
      "Writing short_paragraphs\\ar\\ar_0222.txt\n",
      "Writing short_paragraphs\\ar\\ar_0223.txt\n",
      "Writing short_paragraphs\\ar\\ar_0224.txt\n",
      "Writing short_paragraphs\\ar\\ar_0225.txt\n",
      "Writing paragraphs\\ar\\ar_0023.txt\n",
      "Writing short_paragraphs\\ar\\ar_0226.txt\n",
      "Writing short_paragraphs\\ar\\ar_0227.txt\n",
      "Writing short_paragraphs\\ar\\ar_0228.txt\n",
      "Writing short_paragraphs\\ar\\ar_0229.txt\n",
      "Writing short_paragraphs\\ar\\ar_0230.txt\n",
      "Writing short_paragraphs\\ar\\ar_0231.txt\n",
      "Writing short_paragraphs\\ar\\ar_0232.txt\n",
      "Writing short_paragraphs\\ar\\ar_0233.txt\n",
      "Writing short_paragraphs\\ar\\ar_0234.txt\n",
      "Writing paragraphs\\ar\\ar_0024.txt\n",
      "Writing short_paragraphs\\ar\\ar_0235.txt\n",
      "Writing short_paragraphs\\ar\\ar_0236.txt\n",
      "Writing short_paragraphs\\ar\\ar_0237.txt\n",
      "Writing short_paragraphs\\ar\\ar_0238.txt\n",
      "Writing short_paragraphs\\ar\\ar_0239.txt\n",
      "Writing short_paragraphs\\ar\\ar_0240.txt\n",
      "Writing short_paragraphs\\ar\\ar_0241.txt\n",
      "Writing short_paragraphs\\ar\\ar_0242.txt\n",
      "Writing short_paragraphs\\ar\\ar_0243.txt\n",
      "Writing short_paragraphs\\ar\\ar_0244.txt\n",
      "Writing paragraphs\\ar\\ar_0025.txt\n",
      "Writing short_paragraphs\\ar\\ar_0245.txt\n",
      "Writing short_paragraphs\\ar\\ar_0246.txt\n",
      "Writing short_paragraphs\\ar\\ar_0247.txt\n",
      "Writing short_paragraphs\\ar\\ar_0248.txt\n",
      "Writing short_paragraphs\\ar\\ar_0249.txt\n",
      "Writing short_paragraphs\\ar\\ar_0250.txt\n",
      "Writing short_paragraphs\\ar\\ar_0251.txt\n",
      "Writing short_paragraphs\\ar\\ar_0252.txt\n",
      "Writing short_paragraphs\\ar\\ar_0253.txt\n",
      "Writing short_paragraphs\\ar\\ar_0254.txt\n",
      "Writing short_paragraphs\\ar\\ar_0255.txt\n",
      "Writing short_paragraphs\\ar\\ar_0256.txt\n",
      "Writing short_paragraphs\\ar\\ar_0257.txt\n",
      "Writing short_paragraphs\\ar\\ar_0258.txt\n",
      "Writing short_paragraphs\\ar\\ar_0259.txt\n",
      "Writing short_paragraphs\\ar\\ar_0260.txt\n",
      "Writing short_paragraphs\\ar\\ar_0261.txt\n",
      "Writing short_paragraphs\\ar\\ar_0262.txt\n",
      "Writing short_paragraphs\\ar\\ar_0263.txt\n",
      "Writing short_paragraphs\\ar\\ar_0264.txt\n",
      "Writing short_paragraphs\\ar\\ar_0265.txt\n",
      "Writing short_paragraphs\\ar\\ar_0266.txt\n",
      "Writing short_paragraphs\\ar\\ar_0267.txt\n",
      "Writing short_paragraphs\\ar\\ar_0268.txt\n",
      "Writing short_paragraphs\\ar\\ar_0269.txt\n",
      "Writing short_paragraphs\\ar\\ar_0270.txt\n",
      "Writing short_paragraphs\\ar\\ar_0271.txt\n",
      "Writing short_paragraphs\\ar\\ar_0272.txt\n",
      "Writing short_paragraphs\\ar\\ar_0273.txt\n",
      "Writing short_paragraphs\\ar\\ar_0274.txt\n",
      "Writing short_paragraphs\\ar\\ar_0275.txt\n",
      "Writing short_paragraphs\\ar\\ar_0276.txt\n",
      "Writing paragraphs\\ar\\ar_0026.txt\n",
      "Writing short_paragraphs\\ar\\ar_0277.txt\n",
      "Writing short_paragraphs\\ar\\ar_0278.txt\n",
      "Writing short_paragraphs\\ar\\ar_0279.txt\n",
      "Writing short_paragraphs\\ar\\ar_0280.txt\n",
      "Writing short_paragraphs\\ar\\ar_0281.txt\n",
      "Writing short_paragraphs\\ar\\ar_0282.txt\n",
      "Writing short_paragraphs\\ar\\ar_0283.txt\n",
      "Writing short_paragraphs\\ar\\ar_0284.txt\n",
      "Writing short_paragraphs\\ar\\ar_0285.txt\n",
      "Writing short_paragraphs\\ar\\ar_0286.txt\n",
      "Writing short_paragraphs\\ar\\ar_0287.txt\n",
      "Writing short_paragraphs\\ar\\ar_0288.txt\n",
      "Writing short_paragraphs\\ar\\ar_0289.txt\n",
      "Writing short_paragraphs\\ar\\ar_0290.txt\n",
      "Writing short_paragraphs\\ar\\ar_0291.txt\n",
      "Writing short_paragraphs\\ar\\ar_0292.txt\n",
      "Writing short_paragraphs\\ar\\ar_0293.txt\n",
      "Writing short_paragraphs\\ar\\ar_0294.txt\n",
      "Writing short_paragraphs\\ar\\ar_0295.txt\n",
      "Writing short_paragraphs\\ar\\ar_0296.txt\n",
      "Writing paragraphs\\de\\de_0000.txt\n",
      "Writing short_paragraphs\\de\\de_0000.txt\n",
      "Writing short_paragraphs\\de\\de_0001.txt\n",
      "Writing short_paragraphs\\de\\de_0002.txt\n",
      "Writing short_paragraphs\\de\\de_0003.txt\n",
      "Writing short_paragraphs\\de\\de_0004.txt\n",
      "Writing short_paragraphs\\de\\de_0005.txt\n",
      "Writing short_paragraphs\\de\\de_0006.txt\n",
      "Writing short_paragraphs\\de\\de_0007.txt\n",
      "Writing short_paragraphs\\de\\de_0008.txt\n",
      "Writing short_paragraphs\\de\\de_0009.txt\n",
      "Writing short_paragraphs\\de\\de_0010.txt\n",
      "Writing paragraphs\\de\\de_0001.txt\n",
      "Writing short_paragraphs\\de\\de_0011.txt\n",
      "Writing short_paragraphs\\de\\de_0012.txt\n",
      "Writing short_paragraphs\\de\\de_0013.txt\n",
      "Writing short_paragraphs\\de\\de_0014.txt\n",
      "Writing short_paragraphs\\de\\de_0015.txt\n",
      "Writing short_paragraphs\\de\\de_0016.txt\n",
      "Writing short_paragraphs\\de\\de_0017.txt\n",
      "Writing paragraphs\\de\\de_0002.txt\n",
      "Writing short_paragraphs\\de\\de_0018.txt\n",
      "Writing short_paragraphs\\de\\de_0019.txt\n",
      "Writing short_paragraphs\\de\\de_0020.txt\n",
      "Writing short_paragraphs\\de\\de_0021.txt\n",
      "Writing short_paragraphs\\de\\de_0022.txt\n",
      "Writing short_paragraphs\\de\\de_0023.txt\n",
      "Writing short_paragraphs\\de\\de_0024.txt\n",
      "Writing short_paragraphs\\de\\de_0025.txt\n",
      "Writing short_paragraphs\\de\\de_0026.txt\n",
      "Writing short_paragraphs\\de\\de_0027.txt\n",
      "Writing short_paragraphs\\de\\de_0028.txt\n",
      "Writing short_paragraphs\\de\\de_0029.txt\n",
      "Writing short_paragraphs\\de\\de_0030.txt\n",
      "Writing short_paragraphs\\de\\de_0031.txt\n",
      "Writing paragraphs\\de\\de_0003.txt\n",
      "Writing short_paragraphs\\de\\de_0032.txt\n",
      "Writing short_paragraphs\\de\\de_0033.txt\n",
      "Writing short_paragraphs\\de\\de_0034.txt\n",
      "Writing short_paragraphs\\de\\de_0035.txt\n",
      "Writing short_paragraphs\\de\\de_0036.txt\n",
      "Writing short_paragraphs\\de\\de_0037.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-d062306abd23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Writing %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mshort_text_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             open(short_text_filename, 'wb').write(\n\u001b[1;32m--> 102\u001b[1;33m                 small_content.encode('utf-8', 'ignore'))\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# simple python script to collect text paragraphs from various languages on the\n",
    "# same topic namely the Wikipedia encyclopedia itself\n",
    "\n",
    "import os\n",
    "try:\n",
    "    # Python 2 compat\n",
    "    from urllib2 import Request, build_opener\n",
    "except ImportError:\n",
    "    # Python 3\n",
    "    from urllib.request import Request, build_opener\n",
    "\n",
    "import lxml.html\n",
    "from lxml.etree import ElementTree\n",
    "import numpy as np\n",
    "\n",
    "import codecs\n",
    "\n",
    "pages = {\n",
    "    u'ar': u'http://ar.wikipedia.org/wiki/%D9%88%D9%8A%D9%83%D9%8A%D8%A8%D9%8A%D8%AF%D9%8A%D8%A7',\n",
    "    u'de': u'http://de.wikipedia.org/wiki/Wikipedia',\n",
    "    u'en': u'https://en.wikipedia.org/wiki/Wikipedia',\n",
    "    u'es': u'http://es.wikipedia.org/wiki/Wikipedia',\n",
    "    u'fr': u'http://fr.wikipedia.org/wiki/Wikip%C3%A9dia',\n",
    "    u'it': u'http://it.wikipedia.org/wiki/Wikipedia',\n",
    "    u'ja': u'http://ja.wikipedia.org/wiki/Wikipedia',\n",
    "    u'nl': u'http://nl.wikipedia.org/wiki/Wikipedia',\n",
    "    u'pl': u'http://pl.wikipedia.org/wiki/Wikipedia',\n",
    "    u'pt': u'http://pt.wikipedia.org/wiki/Wikip%C3%A9dia',\n",
    "    u'ru': u'http://ru.wikipedia.org/wiki/%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D1%8F',\n",
    "    u'zh': u'http://zh.wikipedia.org/wiki/Wikipedia',\n",
    "}\n",
    "\n",
    "html_folder = u'html'\n",
    "text_folder = u'paragraphs'\n",
    "short_text_folder = u'short_paragraphs'\n",
    "n_words_per_short_text = 5\n",
    "\n",
    "\n",
    "if not os.path.exists(html_folder):\n",
    "    os.makedirs(html_folder)\n",
    "\n",
    "for lang, page in pages.items():\n",
    "\n",
    "    text_lang_folder = os.path.join(text_folder, lang)\n",
    "    if not os.path.exists(text_lang_folder):\n",
    "        os.makedirs(text_lang_folder)\n",
    "\n",
    "    short_text_lang_folder = os.path.join(short_text_folder, lang)\n",
    "    if not os.path.exists(short_text_lang_folder):\n",
    "        os.makedirs(short_text_lang_folder)\n",
    "\n",
    "    opener = build_opener()\n",
    "    html_filename = os.path.join(html_folder, lang + '.html')\n",
    "    if not os.path.exists(html_filename):\n",
    "        print(\"Downloading %s\" % page)\n",
    "        request = Request(page)\n",
    "        # change the User Agent to avoid being blocked by Wikipedia\n",
    "        # downloading a couple of articles should not be considered abusive\n",
    "        request.add_header('User-Agent', 'OpenAnything/1.0')\n",
    "        html_content = opener.open(request).read()\n",
    "        open(html_filename, 'wb').write(html_content)\n",
    "\n",
    "    # decode the payload explicitly as UTF-8 since lxml is confused for some\n",
    "    # reason\n",
    "    with codecs.open(html_filename,'r','utf-8') as html_file:\n",
    "        html_content = html_file.read()\n",
    "    tree = ElementTree(lxml.html.document_fromstring(html_content))\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in tree.findall('//p'):\n",
    "        content = p.text_content()\n",
    "        if len(content) < 100:\n",
    "            # skip paragraphs that are too short - probably too noisy and not\n",
    "            # representative of the actual language\n",
    "            continue\n",
    "\n",
    "        text_filename = os.path.join(text_lang_folder,\n",
    "                                     '%s_%04d.txt' % (lang, i))\n",
    "        print(\"Writing %s\" % text_filename)\n",
    "        open(text_filename, 'wb').write(content.encode('utf-8', 'ignore'))\n",
    "        i += 1\n",
    "\n",
    "        # split the paragraph into fake smaller paragraphs to make the\n",
    "        # problem harder e.g. more similar to tweets\n",
    "        if lang in ('zh', 'ja'):\n",
    "        # FIXME: whitespace tokenizing does not work on chinese and japanese\n",
    "            continue\n",
    "        words = content.split()\n",
    "        n_groups = len(words) / n_words_per_short_text\n",
    "        if n_groups < 1:\n",
    "            continue\n",
    "        groups = np.array_split(words, n_groups)\n",
    "\n",
    "        for group in groups:\n",
    "            small_content = u\" \".join(group)\n",
    "\n",
    "            short_text_filename = os.path.join(short_text_lang_folder,\n",
    "                                               '%s_%04d.txt' % (lang, j))\n",
    "            print(\"Writing %s\" % short_text_filename)\n",
    "            open(short_text_filename, 'wb').write(\n",
    "                small_content.encode('utf-8', 'ignore'))\n",
    "            j += 1\n",
    "            if j >= 1000:\n",
    "                break\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9758\n"
     ]
    }
   ],
   "source": [
    "fn = os.listdir(\"datafornoah/\")\n",
    "fil = []\n",
    "for folder in fn:\n",
    "    sss = os.listdir(\"datafornoah/\" + folder)\n",
    "    if folder == \"html\":\n",
    "        for subfold in sss:\n",
    "            fil.append(\"datafornoah/\" + folder + \"/\" + subfold)\n",
    "    else:\n",
    "        for subfold in sss:\n",
    "            fff = os.listdir(\"datafornoah/\" + folder + \"/\" + subfold + \"/\")\n",
    "            for file in fff:\n",
    "                fil.append(\"datafornoah/\" + folder + \"/\" + subfold + \"/\" + file)\n",
    "#filenames = [\"datafornoah/paragraphs/ar/ar_0000.txt\", \"datafornoah/paragraphs/ar/ar_0001.txt\"]\n",
    "#for fname in filenames:\n",
    "#    with open(fname, 'r', encoding=\"utf8\") as fin:\n",
    "#        print(fin.read())\n",
    "print(len(fil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'workspace/exercise_01_language_train_model.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run workspace/exercise_01_language_train_model.py data/languages/paragraphs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'datafornoah\\\\paragraphs\\\\ar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-83707cccc3a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlanguages_data_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"datafornoah\"\u001b[0m\u001b[1;31m#sys.argv[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguages_data_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#for fname in fil:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#    with open(fname, 'r', encoding=\"utf8\") as fin:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#        dataset.append(fin.read())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python\\lib\\site-packages\\sklearn\\datasets\\base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'datafornoah\\\\paragraphs\\\\ar'"
     ]
    }
   ],
   "source": [
    "languages_data_folder = \"datafornoah\"#sys.argv[1]\n",
    "dataset = load_files(languages_data_folder, encoding=\"utf8\")\n",
    "#for fname in fil:\n",
    "#    with open(fname, 'r', encoding=\"utf8\") as fin:\n",
    "#        dataset.append(fin.read())\n",
    "#figure out the cause of this permissions error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9758"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-6d9624b76ee2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m docs_train, docs_test, y_train, y_test = train_test_split(\n\u001b[1;32m----> 2\u001b[1;33m     dataset.data, dataset.target, test_size=0.5)\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this error continued to persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chris came up with a solution whereby I run the code from a python file in an ipython console directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is another file that has the rest of the code for this example. It is also on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks again for the great class! :)\n"
     ]
    }
   ],
   "source": [
    "print(\"Thanks again for the great class! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
